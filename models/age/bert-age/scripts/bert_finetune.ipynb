{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dec022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# per tweet\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from myDataSet import MyTokenizerDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from newfit import train, test\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "split_ratio=0.2\n",
    "input_size = 768\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "decay_rate = 0.95\n",
    "\n",
    "\n",
    "\n",
    "trainPath = './data/traintweetswithlabel_1.csv'\n",
    "train_csv=pd.read_csv(trainPath)\n",
    "dfTrain=pd.DataFrame(train_csv)\n",
    "print(\"loading train data\")\n",
    "train_data=MyTokenizerDataset(dfTrain)\n",
    "\n",
    "# train_counts=[pd.value_counts(dfTrain['label'])[0],pd.value_counts(dfTrain['label'])[1]]\n",
    "train_counts=pd.value_counts(dfTrain['label']).tolist()\n",
    "print(train_counts)\n",
    "train_weights= 1./ torch.tensor(train_counts, dtype=torch.float)\n",
    "train_targets = train_data.getLabels()\n",
    "train_samples_weights = train_weights[train_targets]\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(weights=train_samples_weights, num_samples=len(train_samples_weights), replacement=True)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=train_sampler, num_workers=4,pin_memory=True)\n",
    "\n",
    "\n",
    "testPath = './data/testtweetswithlabel_1.csv'\n",
    "test_csv=pd.read_csv(testPath)\n",
    "dfTest=pd.DataFrame(test_csv)\n",
    "print(\"loading test data\")\n",
    "test_data = MyTokenizerDataset(dfTest)\n",
    "\n",
    "# test_counts=[pd.value_counts(dfTest['label'])[0],pd.value_counts(dfTest['label'])[1]]\n",
    "test_counts=pd.value_counts(dfTest['label']).tolist()\n",
    "print(test_counts)\n",
    "test_weights= 1./ torch.tensor(test_counts, dtype=torch.float)\n",
    "test_targets = test_data.getLabels()\n",
    "test_samples_weights = test_weights[test_targets]\n",
    "test_sampler = torch.utils.data.WeightedRandomSampler(weights=test_samples_weights, num_samples=len(test_samples_weights), replacement=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, sampler=test_sampler, num_workers=4,pin_memory=True)\n",
    "\n",
    "lrs=[1e-4,5e-5,2e-5,1e-5,5e-6]\n",
    "for lr in lrs:\n",
    "    learning_rate=lr\n",
    "    net = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.   \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    net.cuda()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=learning_rate,eps=1e-8)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "\n",
    "    test(net,test_loader,device)\n",
    "    train(net, train_loader, test_loader,optimizer, scheduler, device, epochs)\n",
    "    test(net,test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per user\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from myDataSet import MyTokenizerDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fit import train, test\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "split_ratio=0.2\n",
    "input_size = 768\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "decay_rate = 0.95\n",
    "\n",
    "trainPath = './data/mergetraintweetswithlabel_1.csv'\n",
    "train_csv=pd.read_csv(trainPath)\n",
    "dfTrain=pd.DataFrame(train_csv)\n",
    "dfTrainauc=dfTrain.loc[dfTrain['label']==1]\n",
    "dfTrain=pd.concat([dfTrain,dfTrainauc], ignore_index=True)\n",
    "print(\"loading train data\")\n",
    "train_data=MyTokenizerDataset(dfTrain)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "\n",
    "testPath = './data/mergetesttweetswithlabel_1.csv'\n",
    "test_csv=pd.read_csv(testPath)\n",
    "dfTest=pd.DataFrame(test_csv)\n",
    "# dfTestauc=dfTest.loc[dfTest['label']==1]\n",
    "# dfTest=pd.concat([dfTest,dfTestauc], ignore_index=True)\n",
    "print(\"loading test data\")\n",
    "test_data = MyTokenizerDataset(dfTest)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True)\n",
    "\n",
    "lrs=[1e-7,5e-6]\n",
    "for lr in lrs:\n",
    "    learning_rate=lr\n",
    "    net = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.   \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    net.cuda()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=learning_rate,eps=1e-8)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "\n",
    "    test(net,test_loader,device)\n",
    "    train(net, train_loader, test_loader,optimizer, scheduler, device, epochs)\n",
    "    test(net,test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc20825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}