{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fcac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load bertFt.py\n",
    "\"\"\"\n",
    "    per tweet\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from myDataSet import MyTokenizerDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fit import train, test\n",
    "import utils\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "split_ratio=0.2\n",
    "input_size = 768\n",
    "epochs = 10\n",
    "batch_size = 1024\n",
    "decay_rate = 0.95\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # since only two classes in train data\n",
    "    # use sigmoid to classify\n",
    "    def __init__(self, inSize, classes=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(inSize, int(inSize/2), 2, batch_first=True,bidirectional=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc1 = nn.Linear(inSize, 256)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.act2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(int(inSize/2), classes)\n",
    "        if classes < 2:\n",
    "            self.classify = nn.Sigmoid()\n",
    "        else:\n",
    "            self.classify = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "#         x=x.view(len(x), 1, -1) \n",
    "        lstm_out, (hidden_last,cn_last) = self.lstm(x, hidden)\n",
    "        hidden_last_out=hidden_last[-1]\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.act1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.act2(out)\n",
    "        out = self.fc3(hidden_last_out)\n",
    "        out = self.classify(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        number = 1\n",
    "\n",
    "        hidden = (weight.new(2*number, batch_size, 384).zero_().float().cuda(),\n",
    "                  weight.new(2*number, batch_size, 384).zero_().float().cuda()\n",
    "                 )\n",
    "\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    \n",
    "trainPath = './data/traintweetswithlabel_1.csv'\n",
    "train_csv=pd.read_csv(trainPath)\n",
    "dfTrain=pd.DataFrame(train_csv)\n",
    "dfTrainauc=dfTrain.loc[dfTrain['label']==1]\n",
    "dfTrain=pd.concat([dfTrain,dfTrainauc], ignore_index=True)\n",
    "train_counts=pd.value_counts(dfTrain['label']).tolist()\n",
    "print(train_counts)\n",
    "train_data=MyTokenizerDataset(dfTrain)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True,drop_last=True)\n",
    "\n",
    "# train_counts=[pd.value_counts(dfTrain['label'])[0],pd.value_counts(dfTrain['label'])[1]]\n",
    "# train_counts=pd.value_counts(dfTrain['label']).tolist()\n",
    "# print(train_counts)\n",
    "# train_weights= 1./ torch.tensor(train_counts, dtype=torch.float)\n",
    "# train_targets = train_data.getLabels()\n",
    "# train_samples_weights = train_weights[train_targets]\n",
    "# train_sampler = torch.utils.data.WeightedRandomSampler(weights=train_samples_weights, num_samples=len(train_samples_weights), replacement=True)\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=train_sampler, num_workers=4,pin_memory=True)\n",
    "\n",
    "\n",
    "testPath = './data/testtweetswithlabel_1.csv'\n",
    "test_csv=pd.read_csv(testPath)\n",
    "dfTest=pd.DataFrame(test_csv)\n",
    "test_data = MyTokenizerDataset(dfTest)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True,drop_last=True)\n",
    "\n",
    "\n",
    "# test_counts=[pd.value_counts(dfTest['label'])[0],pd.value_counts(dfTest['label'])[1]]\n",
    "# test_counts=pd.value_counts(dfTest['label']).tolist()\n",
    "# print(test_counts)\n",
    "# test_weights= 1./ torch.tensor(test_counts, dtype=torch.float)\n",
    "# test_targets = test_data.getLabels()\n",
    "# test_samples_weights = test_weights[test_targets]\n",
    "# test_sampler = torch.utils.data.WeightedRandomSampler(weights=test_samples_weights, num_samples=len(test_samples_weights), replacement=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, sampler=test_sampler, num_workers=4,pin_memory=True)\n",
    "\n",
    "# test_size=int(len(data)*split_ratio)\n",
    "# train_size=len(data)-test_size\n",
    "# train_data, test_data = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "num_features = bert.pooler.dense.in_features\n",
    "\n",
    "net = Net(num_features, 1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = utils.BCEFocalLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "# train(net, train_loader, test_loader, bert, tokenizer, optimizer, criterion, scheduler, device, epochs)\n",
    "# test(net, test_loader, bert, tokenizer, criterion, device)\n",
    "test(net,test_loader,bert,criterion,device,batch_size)\n",
    "train(net, train_loader, test_loader, bert, optimizer, criterion, scheduler, device, epochs,batch_size)\n",
    "test(net,test_loader,bert,criterion,device,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0183a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de18ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load bertFt.py\n",
    "\"\"\"\n",
    "    per user\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from myDataSet import MyTokenizerDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fit import train, test\n",
    "import utils\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-7\n",
    "split_ratio=0.2\n",
    "input_size = 768\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "decay_rate = 0.95\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # since only two classes in train data\n",
    "    # use sigmoid to classify\n",
    "    def __init__(self, inSize, classes=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(inSize, int(inSize/2), 2, batch_first=True,bidirectional=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc1 = nn.Linear(inSize, 256)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(256, 64)\n",
    "#         self.act2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(int(inSize/2), classes)\n",
    "        if classes < 2:\n",
    "            self.classify = nn.Sigmoid()\n",
    "        else:\n",
    "            self.classify = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "#         x=x.view(len(x), 1, -1) \n",
    "        lstm_out, (hidden_last,cn_last) = self.lstm(x, hidden)\n",
    "        hidden_last_out=hidden_last[-1]\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.act1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.act2(out)\n",
    "        out = self.fc3(hidden_last_out)\n",
    "        out = self.classify(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        number = 1\n",
    "#         if self.bidirectional:\n",
    "#             number = 2\n",
    "        \n",
    "\n",
    "        hidden = (weight.new(2*number, batch_size, 384).zero_().float().cuda(),\n",
    "                  weight.new(2*number, batch_size, 384).zero_().float().cuda()\n",
    "                 )\n",
    "\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    \n",
    "trainPath = './data/mergetraintweetswithlabel_1.csv'\n",
    "train_csv=pd.read_csv(trainPath)\n",
    "dfTrain=pd.DataFrame(train_csv)\n",
    "\n",
    "dfTrainauc=dfTrain.loc[dfTrain['label']==1]\n",
    "dfTrain=pd.concat([dfTrain,dfTrainauc], ignore_index=True)\n",
    "dfTrain=dfTrain.sample(frac=1).reset_index(drop=True)\n",
    "train_counts=pd.value_counts(dfTrain['label']).tolist()\n",
    "print(train_counts)\n",
    "train_data=MyTokenizerDataset(dfTrain)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True,drop_last=True)\n",
    "\n",
    "# train_counts=[pd.value_counts(dfTrain['label'])[0],pd.value_counts(dfTrain['label'])[1]]\n",
    "# train_counts=pd.value_counts(dfTrain['label']).tolist()\n",
    "# print(train_counts)\n",
    "# train_weights= 1./ torch.tensor(train_counts, dtype=torch.float)\n",
    "# train_targets = train_data.getLabels()\n",
    "# train_samples_weights = train_weights[train_targets]\n",
    "# train_sampler = torch.utils.data.WeightedRandomSampler(weights=train_samples_weights, num_samples=len(train_samples_weights), replacement=True)\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, sampler=train_sampler, num_workers=4,pin_memory=True)\n",
    "\n",
    "\n",
    "testPath = './data/mergetesttweetswithlabel_1.csv'\n",
    "test_csv=pd.read_csv(testPath)\n",
    "dfTest=pd.DataFrame(test_csv)\n",
    "test_data = MyTokenizerDataset(dfTest)\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4,pin_memory=True,drop_last=True)\n",
    "\n",
    "\n",
    "# test_counts=[pd.value_counts(dfTest['label'])[0],pd.value_counts(dfTest['label'])[1]]\n",
    "# test_counts=pd.value_counts(dfTest['label']).tolist()\n",
    "# print(test_counts)\n",
    "# test_weights= 1./ torch.tensor(test_counts, dtype=torch.float)\n",
    "# test_targets = test_data.getLabels()\n",
    "# test_samples_weights = test_weights[test_targets]\n",
    "# test_sampler = torch.utils.data.WeightedRandomSampler(weights=test_samples_weights, num_samples=len(test_samples_weights), replacement=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, sampler=test_sampler, num_workers=4,pin_memory=True)\n",
    "\n",
    "# test_size=int(len(data)*split_ratio)\n",
    "# train_size=len(data)-test_size\n",
    "# train_data, test_data = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "num_features = bert.pooler.dense.in_features\n",
    "\n",
    "net = Net(num_features, 1).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = utils.BCEFocalLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "# train(net, train_loader, test_loader, bert, tokenizer, optimizer, criterion, scheduler, device, epochs)\n",
    "# test(net, test_loader, bert, tokenizer, criterion, device)\n",
    "test(net,test_loader,bert,criterion,device,batch_size)\n",
    "train(net, train_loader, test_loader, bert, optimizer, criterion, scheduler, device, epochs,batch_size)\n",
    "test(net,test_loader,bert,criterion,device,batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6e38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cc44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}