{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2678\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"./labeled_users_1145/tweets.json\", \"r\", encoding='utf-8')\n",
    "data = text_file.read()\n",
    "text_file.close()\n",
    "tweets = json.loads(data)\n",
    "print(len(tweets.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=list(tweets.keys())\n",
    "def remove_punctuation(tweet):\n",
    "    punctuation=[\".\",\",\",\":\",\";\",\"!\",\"?\"]\n",
    "    for p in punctuation:\n",
    "        tweet=tweet.replace(p, \"\")\n",
    "    tweet=tweet.replace(\"@ \",\"@\")\n",
    "    return tweet\n",
    "\n",
    "def extract(tweet):\n",
    "    tweet=remove_punctuation(tweet).lstrip().split(\" \")\n",
    "    tweet=list(filter(lambda x: len(x)>0, tweet))\n",
    "    ats=list(filter(lambda x: x[0]==\"@\", tweet))\n",
    "    hashtags=list(filter(lambda x: x[0]==\"#\",tweet))\n",
    "    clean=list(filter(lambda x: x[0]!=\"@\" and x[0]!=\"#\" and x[0:4]!=\"http\", tweet))\n",
    "    return \" \".join(clean), \" \".join(ats), \" \".join(hashtags)\n",
    "\n",
    "def create_excel(users,indexes):\n",
    "    users=users[indexes[0]:indexes[1]]\n",
    "    df = pd.DataFrame(columns = ['user_id', 'clean_text', '@','#'])\n",
    "    for user in users:\n",
    "        for tweet in tweets[user]:\n",
    "            clean, ats, hashtags = extract(tweet)\n",
    "            df = df.append({'user_id':user, 'clean_text':clean, '@':ats,'#':hashtags},ignore_index=True)\n",
    "\n",
    "    df.to_excel(\"./labeled_users_1145/cleaned_tweets\"+str(indexes)+\".xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[[0,1000],[1000,2000],[2000,len(users)]]\n",
    "create_excel(users, indexes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_excel(users, indexes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_excel(users, indexes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(\"./labeled_users_1145/cleaned_tweets[0, 1000].xlsx\",engine='openpyxl')\n",
    "df2=pd.read_excel(\"./labeled_users_1145/cleaned_tweets[1000, 2000].xlsx\",engine='openpyxl')\n",
    "df3=pd.read_excel(\"./labeled_users_1145/cleaned_tweets[2000, 2678].xlsx\",engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=df1.append(df2, ignore_index = True)\n",
    "merge=merge.append(df3, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.to_excel(\"./labeled_users_1145/cleaned_tweets.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
